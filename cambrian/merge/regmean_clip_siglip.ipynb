{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, CLIPVisionModel, SiglipVisionModel, TrainingArguments, Trainer\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_params_to_merge(param_names, exclude_param_regex):\n",
    "    params_to_merge = []\n",
    "    for name in param_names:\n",
    "        valid = not any([re.match(patt, name) for patt in exclude_param_regex])\n",
    "        if valid:\n",
    "            params_to_merge.append(name)\n",
    "    return params_to_merge\n",
    "\n",
    "\n",
    "def filter_modules_by_regex(base_module, include_patterns, include_type):\n",
    "    modules = {}\n",
    "    for name, module in base_module.named_modules():\n",
    "        valid_name = not include_patterns or any([re.match(patt, name) for patt in include_patterns])\n",
    "        valid_type = not include_type or any([isinstance(module, md_cls) for md_cls in include_type])\n",
    "        if valid_type and valid_name:\n",
    "            modules[name] = module\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ImageNet dataset\n",
    "dataset = load_dataset(\"imagenet-1k\", split=\"train[:32000]\")  # Using a subset for quicker loading\n",
    "\n",
    "# Load processors\n",
    "clip_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "siglip_processor = AutoImageProcessor.from_pretrained(\"google/siglip-large-patch16-384\")\n",
    "\n",
    "# Create a custom dataset class\n",
    "class ImageNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
    "        return {'pixel_values': processed['pixel_values'].squeeze()}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "clip_dataset = ImageNetDataset(dataset, clip_processor)\n",
    "clip_dataloader = DataLoader(clip_dataset, batch_size=512, shuffle=True, num_workers=32)\n",
    "\n",
    "siglip_dataset = ImageNetDataset(dataset, siglip_processor)\n",
    "siglip_dataloader = DataLoader(siglip_dataset, batch_size=512, shuffle=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(model, dataloader):\n",
    "    grams = {} # gram matrices for each linear layer inputs\n",
    "    xn = {} # number of examples used for computing gram\n",
    "\n",
    "    def get_gram(name):\n",
    "        def hook(module, input, output):\n",
    "            x = input[0].detach() # $[b,t,h]\n",
    "            x = x.view(-1, x.size(-1))\n",
    "            xtx = torch.matmul(x.transpose(0,1), x) # [h,h]\n",
    "            if name not in grams:\n",
    "                grams[name] = xtx / x.size(0)\n",
    "                xn[name] = x.size(0)\n",
    "            else:\n",
    "                grams[name] = (grams[name] * xn[name] + xtx) / (x.size(0) + xn[name])\n",
    "                xn[name] += x.size(0)\n",
    "        return hook\n",
    "\n",
    "    linear_modules = filter_modules_by_regex(model, None, [nn.Linear])\n",
    "    handles = []\n",
    "    for name, module in linear_modules.items():\n",
    "        handle = module.register_forward_hook(get_gram(name))\n",
    "        handles.append(handle)\n",
    "\n",
    "    n_step = 30\n",
    "    total = n_step if n_step > 0 else len(dataloader)\n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=total, desc='Computing gram matrix'):\n",
    "        if n_step > 0 and step == n_step:\n",
    "            break\n",
    "\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SiglipVisionModel.from_pretrained('google/siglip-large-patch16-384').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gram matrix:   0%|          | 0/30 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.13 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     grams1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mcompute_gram\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m handles:\n\u001b[1;32m     34\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:925\u001b[0m, in \u001b[0;36mCLIPVisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled CLS states\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    923\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:852\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    849\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    850\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    860\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:637\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    630\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    631\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m         output_attentions,\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:374\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    371\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    373\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 374\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    382\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:265\u001b[0m, in \u001b[0;36mCLIPAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    264\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m--> 265\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m proj_shape \u001b[38;5;241m=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    268\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(query_states, tgt_len, bsz)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mproj_shape)\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:249\u001b[0m, in \u001b[0;36mCLIPAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.13 GiB. GPU "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grams1 = compute_gram(model1, clip_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gram matrix:   0%|          | 0/30 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_810165/344727276.py\", line 19, in __getitem__\n    processed = self.processor(images=image, return_tensors=\"pt\")\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/image_processing_utils.py\", line 41, in __call__\n    return self.preprocess(images, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py\", line 233, in preprocess\n    input_data_format = infer_channel_dimension_format(images[0])\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/image_utils.py\", line 244, in infer_channel_dimension_format\n    raise ValueError(f\"Unsupported number of image dimensions: {image.ndim}\")\nValueError: Unsupported number of image dimensions: 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     grams2 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msiglip_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mcompute_gram\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     24\u001b[0m n_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     25\u001b[0m total \u001b[38;5;241m=\u001b[39m n_step \u001b[38;5;28;01mif\u001b[39;00m n_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m---> 26\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mComputing gram matrix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_step\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_810165/344727276.py\", line 19, in __getitem__\n    processed = self.processor(images=image, return_tensors=\"pt\")\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/image_processing_utils.py\", line 41, in __call__\n    return self.preprocess(images, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/models/siglip/image_processing_siglip.py\", line 233, in preprocess\n    input_data_format = infer_channel_dimension_format(images[0])\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/transformers/image_utils.py\", line 244, in infer_channel_dimension_format\n    raise ValueError(f\"Unsupported number of image dimensions: {image.ndim}\")\nValueError: Unsupported number of image dimensions: 2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grams2 = compute_gram(model2, siglip_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vision_model.encoder.layers.0.self_attn.q_proj', 'vision_model.encoder.layers.0.self_attn.k_proj', 'vision_model.encoder.layers.0.self_attn.v_proj', 'vision_model.encoder.layers.0.self_attn.out_proj', 'vision_model.encoder.layers.0.mlp.fc1', 'vision_model.encoder.layers.0.mlp.fc2', 'vision_model.encoder.layers.1.self_attn.q_proj', 'vision_model.encoder.layers.1.self_attn.k_proj', 'vision_model.encoder.layers.1.self_attn.v_proj', 'vision_model.encoder.layers.1.self_attn.out_proj', 'vision_model.encoder.layers.1.mlp.fc1', 'vision_model.encoder.layers.1.mlp.fc2', 'vision_model.encoder.layers.2.self_attn.q_proj', 'vision_model.encoder.layers.2.self_attn.k_proj', 'vision_model.encoder.layers.2.self_attn.v_proj', 'vision_model.encoder.layers.2.self_attn.out_proj', 'vision_model.encoder.layers.2.mlp.fc1', 'vision_model.encoder.layers.2.mlp.fc2', 'vision_model.encoder.layers.3.self_attn.q_proj', 'vision_model.encoder.layers.3.self_attn.k_proj', 'vision_model.encoder.layers.3.self_attn.v_proj', 'vision_model.encoder.layers.3.self_attn.out_proj', 'vision_model.encoder.layers.3.mlp.fc1', 'vision_model.encoder.layers.3.mlp.fc2', 'vision_model.encoder.layers.4.self_attn.q_proj', 'vision_model.encoder.layers.4.self_attn.k_proj', 'vision_model.encoder.layers.4.self_attn.v_proj', 'vision_model.encoder.layers.4.self_attn.out_proj', 'vision_model.encoder.layers.4.mlp.fc1', 'vision_model.encoder.layers.4.mlp.fc2', 'vision_model.encoder.layers.5.self_attn.q_proj', 'vision_model.encoder.layers.5.self_attn.k_proj', 'vision_model.encoder.layers.5.self_attn.v_proj', 'vision_model.encoder.layers.5.self_attn.out_proj', 'vision_model.encoder.layers.5.mlp.fc1', 'vision_model.encoder.layers.5.mlp.fc2', 'vision_model.encoder.layers.6.self_attn.q_proj', 'vision_model.encoder.layers.6.self_attn.k_proj', 'vision_model.encoder.layers.6.self_attn.v_proj', 'vision_model.encoder.layers.6.self_attn.out_proj', 'vision_model.encoder.layers.6.mlp.fc1', 'vision_model.encoder.layers.6.mlp.fc2', 'vision_model.encoder.layers.7.self_attn.q_proj', 'vision_model.encoder.layers.7.self_attn.k_proj', 'vision_model.encoder.layers.7.self_attn.v_proj', 'vision_model.encoder.layers.7.self_attn.out_proj', 'vision_model.encoder.layers.7.mlp.fc1', 'vision_model.encoder.layers.7.mlp.fc2', 'vision_model.encoder.layers.8.self_attn.q_proj', 'vision_model.encoder.layers.8.self_attn.k_proj', 'vision_model.encoder.layers.8.self_attn.v_proj', 'vision_model.encoder.layers.8.self_attn.out_proj', 'vision_model.encoder.layers.8.mlp.fc1', 'vision_model.encoder.layers.8.mlp.fc2', 'vision_model.encoder.layers.9.self_attn.q_proj', 'vision_model.encoder.layers.9.self_attn.k_proj', 'vision_model.encoder.layers.9.self_attn.v_proj', 'vision_model.encoder.layers.9.self_attn.out_proj', 'vision_model.encoder.layers.9.mlp.fc1', 'vision_model.encoder.layers.9.mlp.fc2', 'vision_model.encoder.layers.10.self_attn.q_proj', 'vision_model.encoder.layers.10.self_attn.k_proj', 'vision_model.encoder.layers.10.self_attn.v_proj', 'vision_model.encoder.layers.10.self_attn.out_proj', 'vision_model.encoder.layers.10.mlp.fc1', 'vision_model.encoder.layers.10.mlp.fc2', 'vision_model.encoder.layers.11.self_attn.q_proj', 'vision_model.encoder.layers.11.self_attn.k_proj', 'vision_model.encoder.layers.11.self_attn.v_proj', 'vision_model.encoder.layers.11.self_attn.out_proj', 'vision_model.encoder.layers.11.mlp.fc1', 'vision_model.encoder.layers.11.mlp.fc2', 'vision_model.encoder.layers.12.self_attn.q_proj', 'vision_model.encoder.layers.12.self_attn.k_proj', 'vision_model.encoder.layers.12.self_attn.v_proj', 'vision_model.encoder.layers.12.self_attn.out_proj', 'vision_model.encoder.layers.12.mlp.fc1', 'vision_model.encoder.layers.12.mlp.fc2', 'vision_model.encoder.layers.13.self_attn.q_proj', 'vision_model.encoder.layers.13.self_attn.k_proj', 'vision_model.encoder.layers.13.self_attn.v_proj', 'vision_model.encoder.layers.13.self_attn.out_proj', 'vision_model.encoder.layers.13.mlp.fc1', 'vision_model.encoder.layers.13.mlp.fc2', 'vision_model.encoder.layers.14.self_attn.q_proj', 'vision_model.encoder.layers.14.self_attn.k_proj', 'vision_model.encoder.layers.14.self_attn.v_proj', 'vision_model.encoder.layers.14.self_attn.out_proj', 'vision_model.encoder.layers.14.mlp.fc1', 'vision_model.encoder.layers.14.mlp.fc2', 'vision_model.encoder.layers.15.self_attn.q_proj', 'vision_model.encoder.layers.15.self_attn.k_proj', 'vision_model.encoder.layers.15.self_attn.v_proj', 'vision_model.encoder.layers.15.self_attn.out_proj', 'vision_model.encoder.layers.15.mlp.fc1', 'vision_model.encoder.layers.15.mlp.fc2', 'vision_model.encoder.layers.16.self_attn.q_proj', 'vision_model.encoder.layers.16.self_attn.k_proj', 'vision_model.encoder.layers.16.self_attn.v_proj', 'vision_model.encoder.layers.16.self_attn.out_proj', 'vision_model.encoder.layers.16.mlp.fc1', 'vision_model.encoder.layers.16.mlp.fc2', 'vision_model.encoder.layers.17.self_attn.q_proj', 'vision_model.encoder.layers.17.self_attn.k_proj', 'vision_model.encoder.layers.17.self_attn.v_proj', 'vision_model.encoder.layers.17.self_attn.out_proj', 'vision_model.encoder.layers.17.mlp.fc1', 'vision_model.encoder.layers.17.mlp.fc2', 'vision_model.encoder.layers.18.self_attn.q_proj', 'vision_model.encoder.layers.18.self_attn.k_proj', 'vision_model.encoder.layers.18.self_attn.v_proj', 'vision_model.encoder.layers.18.self_attn.out_proj', 'vision_model.encoder.layers.18.mlp.fc1', 'vision_model.encoder.layers.18.mlp.fc2', 'vision_model.encoder.layers.19.self_attn.q_proj', 'vision_model.encoder.layers.19.self_attn.k_proj', 'vision_model.encoder.layers.19.self_attn.v_proj', 'vision_model.encoder.layers.19.self_attn.out_proj', 'vision_model.encoder.layers.19.mlp.fc1', 'vision_model.encoder.layers.19.mlp.fc2', 'vision_model.encoder.layers.20.self_attn.q_proj', 'vision_model.encoder.layers.20.self_attn.k_proj', 'vision_model.encoder.layers.20.self_attn.v_proj', 'vision_model.encoder.layers.20.self_attn.out_proj', 'vision_model.encoder.layers.20.mlp.fc1', 'vision_model.encoder.layers.20.mlp.fc2', 'vision_model.encoder.layers.21.self_attn.q_proj', 'vision_model.encoder.layers.21.self_attn.k_proj', 'vision_model.encoder.layers.21.self_attn.v_proj', 'vision_model.encoder.layers.21.self_attn.out_proj', 'vision_model.encoder.layers.21.mlp.fc1', 'vision_model.encoder.layers.21.mlp.fc2', 'vision_model.encoder.layers.22.self_attn.q_proj', 'vision_model.encoder.layers.22.self_attn.k_proj', 'vision_model.encoder.layers.22.self_attn.v_proj', 'vision_model.encoder.layers.22.self_attn.out_proj', 'vision_model.encoder.layers.22.mlp.fc1', 'vision_model.encoder.layers.22.mlp.fc2', 'vision_model.encoder.layers.23.self_attn.q_proj', 'vision_model.encoder.layers.23.self_attn.k_proj', 'vision_model.encoder.layers.23.self_attn.v_proj', 'vision_model.encoder.layers.23.self_attn.out_proj', 'vision_model.encoder.layers.23.mlp.fc1', 'vision_model.encoder.layers.23.mlp.fc2'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder.layer.0.attention.attention.query', 'encoder.layer.0.attention.attention.key', 'encoder.layer.0.attention.attention.value', 'encoder.layer.0.attention.output.dense', 'encoder.layer.0.mlp.fc1', 'encoder.layer.0.mlp.fc2', 'encoder.layer.1.attention.attention.query', 'encoder.layer.1.attention.attention.key', 'encoder.layer.1.attention.attention.value', 'encoder.layer.1.attention.output.dense', 'encoder.layer.1.mlp.fc1', 'encoder.layer.1.mlp.fc2', 'encoder.layer.2.attention.attention.query', 'encoder.layer.2.attention.attention.key', 'encoder.layer.2.attention.attention.value', 'encoder.layer.2.attention.output.dense', 'encoder.layer.2.mlp.fc1', 'encoder.layer.2.mlp.fc2', 'encoder.layer.3.attention.attention.query', 'encoder.layer.3.attention.attention.key', 'encoder.layer.3.attention.attention.value', 'encoder.layer.3.attention.output.dense', 'encoder.layer.3.mlp.fc1', 'encoder.layer.3.mlp.fc2', 'encoder.layer.4.attention.attention.query', 'encoder.layer.4.attention.attention.key', 'encoder.layer.4.attention.attention.value', 'encoder.layer.4.attention.output.dense', 'encoder.layer.4.mlp.fc1', 'encoder.layer.4.mlp.fc2', 'encoder.layer.5.attention.attention.query', 'encoder.layer.5.attention.attention.key', 'encoder.layer.5.attention.attention.value', 'encoder.layer.5.attention.output.dense', 'encoder.layer.5.mlp.fc1', 'encoder.layer.5.mlp.fc2', 'encoder.layer.6.attention.attention.query', 'encoder.layer.6.attention.attention.key', 'encoder.layer.6.attention.attention.value', 'encoder.layer.6.attention.output.dense', 'encoder.layer.6.mlp.fc1', 'encoder.layer.6.mlp.fc2', 'encoder.layer.7.attention.attention.query', 'encoder.layer.7.attention.attention.key', 'encoder.layer.7.attention.attention.value', 'encoder.layer.7.attention.output.dense', 'encoder.layer.7.mlp.fc1', 'encoder.layer.7.mlp.fc2', 'encoder.layer.8.attention.attention.query', 'encoder.layer.8.attention.attention.key', 'encoder.layer.8.attention.attention.value', 'encoder.layer.8.attention.output.dense', 'encoder.layer.8.mlp.fc1', 'encoder.layer.8.mlp.fc2', 'encoder.layer.9.attention.attention.query', 'encoder.layer.9.attention.attention.key', 'encoder.layer.9.attention.attention.value', 'encoder.layer.9.attention.output.dense', 'encoder.layer.9.mlp.fc1', 'encoder.layer.9.mlp.fc2', 'encoder.layer.10.attention.attention.query', 'encoder.layer.10.attention.attention.key', 'encoder.layer.10.attention.attention.value', 'encoder.layer.10.attention.output.dense', 'encoder.layer.10.mlp.fc1', 'encoder.layer.10.mlp.fc2', 'encoder.layer.11.attention.attention.query', 'encoder.layer.11.attention.attention.key', 'encoder.layer.11.attention.attention.value', 'encoder.layer.11.attention.output.dense', 'encoder.layer.11.mlp.fc1', 'encoder.layer.11.mlp.fc2', 'encoder.layer.12.attention.attention.query', 'encoder.layer.12.attention.attention.key', 'encoder.layer.12.attention.attention.value', 'encoder.layer.12.attention.output.dense', 'encoder.layer.12.mlp.fc1', 'encoder.layer.12.mlp.fc2', 'encoder.layer.13.attention.attention.query', 'encoder.layer.13.attention.attention.key', 'encoder.layer.13.attention.attention.value', 'encoder.layer.13.attention.output.dense', 'encoder.layer.13.mlp.fc1', 'encoder.layer.13.mlp.fc2', 'encoder.layer.14.attention.attention.query', 'encoder.layer.14.attention.attention.key', 'encoder.layer.14.attention.attention.value', 'encoder.layer.14.attention.output.dense', 'encoder.layer.14.mlp.fc1', 'encoder.layer.14.mlp.fc2', 'encoder.layer.15.attention.attention.query', 'encoder.layer.15.attention.attention.key', 'encoder.layer.15.attention.attention.value', 'encoder.layer.15.attention.output.dense', 'encoder.layer.15.mlp.fc1', 'encoder.layer.15.mlp.fc2', 'encoder.layer.16.attention.attention.query', 'encoder.layer.16.attention.attention.key', 'encoder.layer.16.attention.attention.value', 'encoder.layer.16.attention.output.dense', 'encoder.layer.16.mlp.fc1', 'encoder.layer.16.mlp.fc2', 'encoder.layer.17.attention.attention.query', 'encoder.layer.17.attention.attention.key', 'encoder.layer.17.attention.attention.value', 'encoder.layer.17.attention.output.dense', 'encoder.layer.17.mlp.fc1', 'encoder.layer.17.mlp.fc2', 'encoder.layer.18.attention.attention.query', 'encoder.layer.18.attention.attention.key', 'encoder.layer.18.attention.attention.value', 'encoder.layer.18.attention.output.dense', 'encoder.layer.18.mlp.fc1', 'encoder.layer.18.mlp.fc2', 'encoder.layer.19.attention.attention.query', 'encoder.layer.19.attention.attention.key', 'encoder.layer.19.attention.attention.value', 'encoder.layer.19.attention.output.dense', 'encoder.layer.19.mlp.fc1', 'encoder.layer.19.mlp.fc2', 'encoder.layer.20.attention.attention.query', 'encoder.layer.20.attention.attention.key', 'encoder.layer.20.attention.attention.value', 'encoder.layer.20.attention.output.dense', 'encoder.layer.20.mlp.fc1', 'encoder.layer.20.mlp.fc2', 'encoder.layer.21.attention.attention.query', 'encoder.layer.21.attention.attention.key', 'encoder.layer.21.attention.attention.value', 'encoder.layer.21.attention.output.dense', 'encoder.layer.21.mlp.fc1', 'encoder.layer.21.mlp.fc2', 'encoder.layer.22.attention.attention.query', 'encoder.layer.22.attention.attention.key', 'encoder.layer.22.attention.attention.value', 'encoder.layer.22.attention.output.dense', 'encoder.layer.22.mlp.fc1', 'encoder.layer.22.mlp.fc2', 'encoder.layer.23.attention.attention.query', 'encoder.layer.23.attention.attention.key', 'encoder.layer.23.attention.attention.value', 'encoder.layer.23.attention.output.dense', 'encoder.layer.23.mlp.fc1', 'encoder.layer.23.mlp.fc2'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2Attention(\n",
       "          (attention): Dinov2SelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model.pre_layrnorm.weight\n",
      "vision_model.pre_layrnorm.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in n2p.items():\n",
    "    if 'layrnorm' in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.projection.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in n2p.items():\n",
    "    if 'projection.bias' in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2p1 = {k:v for k,v in model1.named_parameters()}\n",
    "n2p2 = {k:v for k,v in model2.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = filter_params_to_merge([n for n in n2p1], ['.*classifier.*', '.*projection.bias', '.*layrnorm.*', '.*layernorm.*', '.*scale.*', '.*class_embedding', '.*position_embedding.*', '.*mask_token.*', '.*cls_token.*'])\n",
    "n2 = filter_params_to_merge([n for n in n2p2], ['.*classifier.*', '.*projection.bias', '.*layrnorm.*', '.*layernorm.*', '.*scale.*', '.*class_embedding', '.*position_embedding.*', '.*mask_token.*', '.*cls_token.*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = []\n",
    "for n in n2:\n",
    "    n_new.append(get_clip_name(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = sorted(n1)\n",
    "n_new = sorted(n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if n1 and n_new are the same\n",
    "for i in range(len(n1)):\n",
    "    if n1[i] != n_new[i]:\n",
    "        print(n1[i], n_new[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 == n_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision_model.encoder.layers.23.mlp.fc1.weight'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clip_name('encoder.layer.23.mlp.fc1.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_name(dinov2_name):\n",
    "    if 'embeddings.patch_embeddings.projection' in dinov2_name:\n",
    "        return dinov2_name.replace('embeddings.patch_embeddings.projection', 'vision_model.embeddings.patch_embedding')\n",
    "    if 'encoder.layer' in dinov2_name:\n",
    "        dinov2_name = dinov2_name.replace('encoder.layer', 'vision_model.encoder.layers')\n",
    "        if 'attention' in dinov2_name:\n",
    "            dinov2_name = dinov2_name.replace('attention', 'self_attn', 1)\n",
    "            if 'attention.query' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('attention.query', 'q_proj')\n",
    "            elif 'attention.key' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('attention.key', 'k_proj')\n",
    "            elif 'attention.value' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('attention.value', 'v_proj')\n",
    "            elif 'output.dense' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('output.dense', 'out_proj')\n",
    "            return dinov2_name\n",
    "        elif 'norm1' in dinov2_name:\n",
    "            return dinov2_name.replace('norm1', 'layer_norm1')\n",
    "        elif 'norm2' in dinov2_name:\n",
    "            return dinov2_name.replace('norm2', 'layer_norm2')\n",
    "        return dinov2_name\n",
    "    return dinov2_name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_merge(local_models, global_model, regmean_grams=None, **kwargs):\n",
    "    params = {}\n",
    "    for i, local_model in enumerate(local_models):\n",
    "        \n",
    "        n2p = {k: v for k,v in local_model.named_parameters()}\n",
    "        merge_param_names = filter_params_to_merge([n for n in n2p], ['.*classifier.*', '.*projection.bias', '.*layrnorm.*', '.*layernorm.*', '.*scale.*', '.*class_embedding', '.*position_embedding.*', '.*mask_token.*', '.*cls_token.*']) # for glue label spaces are different\n",
    "        for n in merge_param_names:\n",
    "            # if it is the second model, the second model should be dinov2\n",
    "            if i == 1:\n",
    "                n_clip = get_clip_name(n)\n",
    "                params[n_clip].append(n2p[n])\n",
    "            else:\n",
    "                if n not in params:\n",
    "                    params[n] = []\n",
    "                params[n].append(n2p[n])\n",
    "    \n",
    "    # we also need to modify the name of the parameters in the regmean grams\n",
    "    if regmean_grams:\n",
    "        for k in list(regmean_grams[1].keys()):\n",
    "            new_k = get_clip_name(k)\n",
    "            regmean_grams[1][new_k] = regmean_grams[1].pop(k)\n",
    "\n",
    "    if regmean_grams: # regmean average\n",
    "        avg_params = regmean_merge(params, regmean_grams)\n",
    "\n",
    "    else: # simple average\n",
    "        avg_params = {k: torch.stack(v,0).mean(0) for k, v in params.items()}\n",
    "\n",
    "    return avg_params\n",
    "\n",
    "def copy_params_to_model(avg_params, model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if n in avg_params:\n",
    "            p.data.copy_(avg_params[n])\n",
    "\n",
    "def reduce_non_diag(cov_mat, a):\n",
    "    diag_weight = torch.diag(torch.ones(cov_mat.size(0)) - a).to(cov_mat.device)\n",
    "    non_diag_weight = torch.zeros_like(diag_weight).fill_(a)\n",
    "    weight = diag_weight + non_diag_weight\n",
    "    ret = cov_mat * weight\n",
    "    return ret\n",
    "\n",
    "def regmean_merge(all_params, all_grams):\n",
    "    avg_params = {}\n",
    "    n_model = len(all_grams)\n",
    "    for name in all_params:\n",
    "        h_avged = False\n",
    "        if name.endswith('.weight'):\n",
    "            print(f'Regmean: {name}')\n",
    "            module_name = name[:-len('.weight')]\n",
    "            if module_name in all_grams[0]:\n",
    "                gram_m_ws, grams = [], []\n",
    "\n",
    "                for model_id, model_grams in enumerate(all_grams):\n",
    "                    param_grams = model_grams[module_name]\n",
    "\n",
    "                    # for roberta we dont need this; but it is important for deberta and t5\n",
    "                    #param_grams = reduce_non_diag(param_grams, a=0.9)\n",
    "\n",
    "                    param = all_params[name][model_id]\n",
    "                    gram_m_ws.append(torch.matmul(param_grams, param.transpose(0,1)))\n",
    "                    grams.append(param_grams)\n",
    "                sum_gram = sum(grams)\n",
    "                sum_gram_m_ws = sum(gram_m_ws)\n",
    "                sum_gram_inv = torch.inverse(sum_gram)\n",
    "                wt = torch.matmul(sum_gram_inv, sum_gram_m_ws)\n",
    "                w = wt.transpose(0,1)\n",
    "                avg_params[name] = w\n",
    "                h_avged = True\n",
    "        if not h_avged: # if not averaged with regmean, then do simple avg\n",
    "            avg_params[name] = torch.stack(all_params[name],0).mean(0)\n",
    "           \n",
    "    return avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regmean: vision_model.embeddings.patch_embedding.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.0.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.1.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.2.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.3.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.4.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.5.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.6.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.7.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.8.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.9.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.10.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.11.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.12.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.13.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.14.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.15.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.16.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.17.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.18.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.19.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.20.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.21.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.22.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.23.layer_norm2.weight\n"
     ]
    }
   ],
   "source": [
    "regmean_avg_params = avg_merge([model1, model2], merged_model, regmean_grams=[grams1, grams2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_params_to_model(regmean_avg_params, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13983.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/gavin/cambrian/cambrian/merge\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained('../../checkpoints/regmean_dinov2_clip_vit_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cambrian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
