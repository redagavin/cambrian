{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, CLIPVisionModel, Dinov2Model, TrainingArguments, Trainer\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_params_to_merge(param_names, exclude_param_regex):\n",
    "    params_to_merge = []\n",
    "    for name in param_names:\n",
    "        valid = not any([re.match(patt, name) for patt in exclude_param_regex])\n",
    "        if valid:\n",
    "            params_to_merge.append(name)\n",
    "    return params_to_merge\n",
    "\n",
    "\n",
    "def filter_modules_by_regex(base_module, include_patterns, include_type):\n",
    "    modules = {}\n",
    "    for name, module in base_module.named_modules():\n",
    "        valid_name = not include_patterns or any([re.match(patt, name) for patt in include_patterns])\n",
    "        valid_type = not include_type or any([isinstance(module, md_cls) for md_cls in include_type])\n",
    "        if valid_type and valid_name:\n",
    "            modules[name] = module\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ImageNet dataset\n",
    "dataset = load_dataset(\"imagenet-1k\", split=\"train[:32000]\")  # Using a subset for quicker loading\n",
    "\n",
    "# Load processors\n",
    "clip_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-large\")\n",
    "\n",
    "# Create a custom dataset class\n",
    "class ImageNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
    "        return {'pixel_values': processed['pixel_values'].squeeze()}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "clip_dataset = ImageNetDataset(dataset, clip_processor)\n",
    "clip_dataloader = DataLoader(clip_dataset, batch_size=512, shuffle=True, num_workers=32)\n",
    "\n",
    "dino_dataset = ImageNetDataset(dataset, dino_processor)\n",
    "dino_dataloader = DataLoader(dino_dataset, batch_size=512, shuffle=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(model, dataloader):\n",
    "    grams = {} # gram matrices for each linear layer inputs\n",
    "    xn = {} # number of examples used for computing gram\n",
    "\n",
    "    def get_gram(name):\n",
    "        def hook(module, input, output):\n",
    "            x = input[0].detach() # $[b,t,h]\n",
    "            x = x.view(-1, x.size(-1))\n",
    "            xtx = torch.matmul(x.transpose(0,1), x) # [h,h]\n",
    "            if name not in grams:\n",
    "                grams[name] = xtx / x.size(0)\n",
    "                xn[name] = x.size(0)\n",
    "            else:\n",
    "                grams[name] = (grams[name] * xn[name] + xtx) / (x.size(0) + xn[name])\n",
    "                xn[name] += x.size(0)\n",
    "        return hook\n",
    "\n",
    "    linear_modules = filter_modules_by_regex(model, None, [nn.Linear])\n",
    "    handles = []\n",
    "    for name, module in linear_modules.items():\n",
    "        handle = module.register_forward_hook(get_gram(name))\n",
    "        handles.append(handle)\n",
    "\n",
    "    n_step = 30\n",
    "    total = n_step if n_step > 0 else len(dataloader)\n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=total, desc='Computing gram matrix'):\n",
    "        if n_step > 0 and step == n_step:\n",
    "            break\n",
    "\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Dinov2Model.from_pretrained('facebook/dinov2-large').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gram matrix: 100%|██████████| 30/30 [08:18<00:00, 16.61s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grams1 = compute_gram(model1, clip_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gram matrix: 100%|██████████| 30/30 [03:35<00:00,  7.17s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grams2 = compute_gram(model2, dino_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vision_model.encoder.layers.0.self_attn.q_proj', 'vision_model.encoder.layers.0.self_attn.k_proj', 'vision_model.encoder.layers.0.self_attn.v_proj', 'vision_model.encoder.layers.0.self_attn.out_proj', 'vision_model.encoder.layers.0.mlp.fc1', 'vision_model.encoder.layers.0.mlp.fc2', 'vision_model.encoder.layers.1.self_attn.q_proj', 'vision_model.encoder.layers.1.self_attn.k_proj', 'vision_model.encoder.layers.1.self_attn.v_proj', 'vision_model.encoder.layers.1.self_attn.out_proj', 'vision_model.encoder.layers.1.mlp.fc1', 'vision_model.encoder.layers.1.mlp.fc2', 'vision_model.encoder.layers.2.self_attn.q_proj', 'vision_model.encoder.layers.2.self_attn.k_proj', 'vision_model.encoder.layers.2.self_attn.v_proj', 'vision_model.encoder.layers.2.self_attn.out_proj', 'vision_model.encoder.layers.2.mlp.fc1', 'vision_model.encoder.layers.2.mlp.fc2', 'vision_model.encoder.layers.3.self_attn.q_proj', 'vision_model.encoder.layers.3.self_attn.k_proj', 'vision_model.encoder.layers.3.self_attn.v_proj', 'vision_model.encoder.layers.3.self_attn.out_proj', 'vision_model.encoder.layers.3.mlp.fc1', 'vision_model.encoder.layers.3.mlp.fc2', 'vision_model.encoder.layers.4.self_attn.q_proj', 'vision_model.encoder.layers.4.self_attn.k_proj', 'vision_model.encoder.layers.4.self_attn.v_proj', 'vision_model.encoder.layers.4.self_attn.out_proj', 'vision_model.encoder.layers.4.mlp.fc1', 'vision_model.encoder.layers.4.mlp.fc2', 'vision_model.encoder.layers.5.self_attn.q_proj', 'vision_model.encoder.layers.5.self_attn.k_proj', 'vision_model.encoder.layers.5.self_attn.v_proj', 'vision_model.encoder.layers.5.self_attn.out_proj', 'vision_model.encoder.layers.5.mlp.fc1', 'vision_model.encoder.layers.5.mlp.fc2', 'vision_model.encoder.layers.6.self_attn.q_proj', 'vision_model.encoder.layers.6.self_attn.k_proj', 'vision_model.encoder.layers.6.self_attn.v_proj', 'vision_model.encoder.layers.6.self_attn.out_proj', 'vision_model.encoder.layers.6.mlp.fc1', 'vision_model.encoder.layers.6.mlp.fc2', 'vision_model.encoder.layers.7.self_attn.q_proj', 'vision_model.encoder.layers.7.self_attn.k_proj', 'vision_model.encoder.layers.7.self_attn.v_proj', 'vision_model.encoder.layers.7.self_attn.out_proj', 'vision_model.encoder.layers.7.mlp.fc1', 'vision_model.encoder.layers.7.mlp.fc2', 'vision_model.encoder.layers.8.self_attn.q_proj', 'vision_model.encoder.layers.8.self_attn.k_proj', 'vision_model.encoder.layers.8.self_attn.v_proj', 'vision_model.encoder.layers.8.self_attn.out_proj', 'vision_model.encoder.layers.8.mlp.fc1', 'vision_model.encoder.layers.8.mlp.fc2', 'vision_model.encoder.layers.9.self_attn.q_proj', 'vision_model.encoder.layers.9.self_attn.k_proj', 'vision_model.encoder.layers.9.self_attn.v_proj', 'vision_model.encoder.layers.9.self_attn.out_proj', 'vision_model.encoder.layers.9.mlp.fc1', 'vision_model.encoder.layers.9.mlp.fc2', 'vision_model.encoder.layers.10.self_attn.q_proj', 'vision_model.encoder.layers.10.self_attn.k_proj', 'vision_model.encoder.layers.10.self_attn.v_proj', 'vision_model.encoder.layers.10.self_attn.out_proj', 'vision_model.encoder.layers.10.mlp.fc1', 'vision_model.encoder.layers.10.mlp.fc2', 'vision_model.encoder.layers.11.self_attn.q_proj', 'vision_model.encoder.layers.11.self_attn.k_proj', 'vision_model.encoder.layers.11.self_attn.v_proj', 'vision_model.encoder.layers.11.self_attn.out_proj', 'vision_model.encoder.layers.11.mlp.fc1', 'vision_model.encoder.layers.11.mlp.fc2', 'vision_model.encoder.layers.12.self_attn.q_proj', 'vision_model.encoder.layers.12.self_attn.k_proj', 'vision_model.encoder.layers.12.self_attn.v_proj', 'vision_model.encoder.layers.12.self_attn.out_proj', 'vision_model.encoder.layers.12.mlp.fc1', 'vision_model.encoder.layers.12.mlp.fc2', 'vision_model.encoder.layers.13.self_attn.q_proj', 'vision_model.encoder.layers.13.self_attn.k_proj', 'vision_model.encoder.layers.13.self_attn.v_proj', 'vision_model.encoder.layers.13.self_attn.out_proj', 'vision_model.encoder.layers.13.mlp.fc1', 'vision_model.encoder.layers.13.mlp.fc2', 'vision_model.encoder.layers.14.self_attn.q_proj', 'vision_model.encoder.layers.14.self_attn.k_proj', 'vision_model.encoder.layers.14.self_attn.v_proj', 'vision_model.encoder.layers.14.self_attn.out_proj', 'vision_model.encoder.layers.14.mlp.fc1', 'vision_model.encoder.layers.14.mlp.fc2', 'vision_model.encoder.layers.15.self_attn.q_proj', 'vision_model.encoder.layers.15.self_attn.k_proj', 'vision_model.encoder.layers.15.self_attn.v_proj', 'vision_model.encoder.layers.15.self_attn.out_proj', 'vision_model.encoder.layers.15.mlp.fc1', 'vision_model.encoder.layers.15.mlp.fc2', 'vision_model.encoder.layers.16.self_attn.q_proj', 'vision_model.encoder.layers.16.self_attn.k_proj', 'vision_model.encoder.layers.16.self_attn.v_proj', 'vision_model.encoder.layers.16.self_attn.out_proj', 'vision_model.encoder.layers.16.mlp.fc1', 'vision_model.encoder.layers.16.mlp.fc2', 'vision_model.encoder.layers.17.self_attn.q_proj', 'vision_model.encoder.layers.17.self_attn.k_proj', 'vision_model.encoder.layers.17.self_attn.v_proj', 'vision_model.encoder.layers.17.self_attn.out_proj', 'vision_model.encoder.layers.17.mlp.fc1', 'vision_model.encoder.layers.17.mlp.fc2', 'vision_model.encoder.layers.18.self_attn.q_proj', 'vision_model.encoder.layers.18.self_attn.k_proj', 'vision_model.encoder.layers.18.self_attn.v_proj', 'vision_model.encoder.layers.18.self_attn.out_proj', 'vision_model.encoder.layers.18.mlp.fc1', 'vision_model.encoder.layers.18.mlp.fc2', 'vision_model.encoder.layers.19.self_attn.q_proj', 'vision_model.encoder.layers.19.self_attn.k_proj', 'vision_model.encoder.layers.19.self_attn.v_proj', 'vision_model.encoder.layers.19.self_attn.out_proj', 'vision_model.encoder.layers.19.mlp.fc1', 'vision_model.encoder.layers.19.mlp.fc2', 'vision_model.encoder.layers.20.self_attn.q_proj', 'vision_model.encoder.layers.20.self_attn.k_proj', 'vision_model.encoder.layers.20.self_attn.v_proj', 'vision_model.encoder.layers.20.self_attn.out_proj', 'vision_model.encoder.layers.20.mlp.fc1', 'vision_model.encoder.layers.20.mlp.fc2', 'vision_model.encoder.layers.21.self_attn.q_proj', 'vision_model.encoder.layers.21.self_attn.k_proj', 'vision_model.encoder.layers.21.self_attn.v_proj', 'vision_model.encoder.layers.21.self_attn.out_proj', 'vision_model.encoder.layers.21.mlp.fc1', 'vision_model.encoder.layers.21.mlp.fc2', 'vision_model.encoder.layers.22.self_attn.q_proj', 'vision_model.encoder.layers.22.self_attn.k_proj', 'vision_model.encoder.layers.22.self_attn.v_proj', 'vision_model.encoder.layers.22.self_attn.out_proj', 'vision_model.encoder.layers.22.mlp.fc1', 'vision_model.encoder.layers.22.mlp.fc2', 'vision_model.encoder.layers.23.self_attn.q_proj', 'vision_model.encoder.layers.23.self_attn.k_proj', 'vision_model.encoder.layers.23.self_attn.v_proj', 'vision_model.encoder.layers.23.self_attn.out_proj', 'vision_model.encoder.layers.23.mlp.fc1', 'vision_model.encoder.layers.23.mlp.fc2'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder.layer.0.attention.attention.query', 'encoder.layer.0.attention.attention.key', 'encoder.layer.0.attention.attention.value', 'encoder.layer.0.attention.output.dense', 'encoder.layer.0.mlp.fc1', 'encoder.layer.0.mlp.fc2', 'encoder.layer.1.attention.attention.query', 'encoder.layer.1.attention.attention.key', 'encoder.layer.1.attention.attention.value', 'encoder.layer.1.attention.output.dense', 'encoder.layer.1.mlp.fc1', 'encoder.layer.1.mlp.fc2', 'encoder.layer.2.attention.attention.query', 'encoder.layer.2.attention.attention.key', 'encoder.layer.2.attention.attention.value', 'encoder.layer.2.attention.output.dense', 'encoder.layer.2.mlp.fc1', 'encoder.layer.2.mlp.fc2', 'encoder.layer.3.attention.attention.query', 'encoder.layer.3.attention.attention.key', 'encoder.layer.3.attention.attention.value', 'encoder.layer.3.attention.output.dense', 'encoder.layer.3.mlp.fc1', 'encoder.layer.3.mlp.fc2', 'encoder.layer.4.attention.attention.query', 'encoder.layer.4.attention.attention.key', 'encoder.layer.4.attention.attention.value', 'encoder.layer.4.attention.output.dense', 'encoder.layer.4.mlp.fc1', 'encoder.layer.4.mlp.fc2', 'encoder.layer.5.attention.attention.query', 'encoder.layer.5.attention.attention.key', 'encoder.layer.5.attention.attention.value', 'encoder.layer.5.attention.output.dense', 'encoder.layer.5.mlp.fc1', 'encoder.layer.5.mlp.fc2', 'encoder.layer.6.attention.attention.query', 'encoder.layer.6.attention.attention.key', 'encoder.layer.6.attention.attention.value', 'encoder.layer.6.attention.output.dense', 'encoder.layer.6.mlp.fc1', 'encoder.layer.6.mlp.fc2', 'encoder.layer.7.attention.attention.query', 'encoder.layer.7.attention.attention.key', 'encoder.layer.7.attention.attention.value', 'encoder.layer.7.attention.output.dense', 'encoder.layer.7.mlp.fc1', 'encoder.layer.7.mlp.fc2', 'encoder.layer.8.attention.attention.query', 'encoder.layer.8.attention.attention.key', 'encoder.layer.8.attention.attention.value', 'encoder.layer.8.attention.output.dense', 'encoder.layer.8.mlp.fc1', 'encoder.layer.8.mlp.fc2', 'encoder.layer.9.attention.attention.query', 'encoder.layer.9.attention.attention.key', 'encoder.layer.9.attention.attention.value', 'encoder.layer.9.attention.output.dense', 'encoder.layer.9.mlp.fc1', 'encoder.layer.9.mlp.fc2', 'encoder.layer.10.attention.attention.query', 'encoder.layer.10.attention.attention.key', 'encoder.layer.10.attention.attention.value', 'encoder.layer.10.attention.output.dense', 'encoder.layer.10.mlp.fc1', 'encoder.layer.10.mlp.fc2', 'encoder.layer.11.attention.attention.query', 'encoder.layer.11.attention.attention.key', 'encoder.layer.11.attention.attention.value', 'encoder.layer.11.attention.output.dense', 'encoder.layer.11.mlp.fc1', 'encoder.layer.11.mlp.fc2', 'encoder.layer.12.attention.attention.query', 'encoder.layer.12.attention.attention.key', 'encoder.layer.12.attention.attention.value', 'encoder.layer.12.attention.output.dense', 'encoder.layer.12.mlp.fc1', 'encoder.layer.12.mlp.fc2', 'encoder.layer.13.attention.attention.query', 'encoder.layer.13.attention.attention.key', 'encoder.layer.13.attention.attention.value', 'encoder.layer.13.attention.output.dense', 'encoder.layer.13.mlp.fc1', 'encoder.layer.13.mlp.fc2', 'encoder.layer.14.attention.attention.query', 'encoder.layer.14.attention.attention.key', 'encoder.layer.14.attention.attention.value', 'encoder.layer.14.attention.output.dense', 'encoder.layer.14.mlp.fc1', 'encoder.layer.14.mlp.fc2', 'encoder.layer.15.attention.attention.query', 'encoder.layer.15.attention.attention.key', 'encoder.layer.15.attention.attention.value', 'encoder.layer.15.attention.output.dense', 'encoder.layer.15.mlp.fc1', 'encoder.layer.15.mlp.fc2', 'encoder.layer.16.attention.attention.query', 'encoder.layer.16.attention.attention.key', 'encoder.layer.16.attention.attention.value', 'encoder.layer.16.attention.output.dense', 'encoder.layer.16.mlp.fc1', 'encoder.layer.16.mlp.fc2', 'encoder.layer.17.attention.attention.query', 'encoder.layer.17.attention.attention.key', 'encoder.layer.17.attention.attention.value', 'encoder.layer.17.attention.output.dense', 'encoder.layer.17.mlp.fc1', 'encoder.layer.17.mlp.fc2', 'encoder.layer.18.attention.attention.query', 'encoder.layer.18.attention.attention.key', 'encoder.layer.18.attention.attention.value', 'encoder.layer.18.attention.output.dense', 'encoder.layer.18.mlp.fc1', 'encoder.layer.18.mlp.fc2', 'encoder.layer.19.attention.attention.query', 'encoder.layer.19.attention.attention.key', 'encoder.layer.19.attention.attention.value', 'encoder.layer.19.attention.output.dense', 'encoder.layer.19.mlp.fc1', 'encoder.layer.19.mlp.fc2', 'encoder.layer.20.attention.attention.query', 'encoder.layer.20.attention.attention.key', 'encoder.layer.20.attention.attention.value', 'encoder.layer.20.attention.output.dense', 'encoder.layer.20.mlp.fc1', 'encoder.layer.20.mlp.fc2', 'encoder.layer.21.attention.attention.query', 'encoder.layer.21.attention.attention.key', 'encoder.layer.21.attention.attention.value', 'encoder.layer.21.attention.output.dense', 'encoder.layer.21.mlp.fc1', 'encoder.layer.21.mlp.fc2', 'encoder.layer.22.attention.attention.query', 'encoder.layer.22.attention.attention.key', 'encoder.layer.22.attention.attention.value', 'encoder.layer.22.attention.output.dense', 'encoder.layer.22.mlp.fc1', 'encoder.layer.22.mlp.fc2', 'encoder.layer.23.attention.attention.query', 'encoder.layer.23.attention.attention.key', 'encoder.layer.23.attention.attention.value', 'encoder.layer.23.attention.output.dense', 'encoder.layer.23.mlp.fc1', 'encoder.layer.23.mlp.fc2'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2Attention(\n",
       "          (attention): Dinov2SelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model.pre_layrnorm.weight\n",
      "vision_model.pre_layrnorm.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in n2p.items():\n",
    "    if 'layrnorm' in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.projection.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in n2p.items():\n",
    "    if 'projection.bias' in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2p1 = {k:v for k,v in model1.named_parameters()}\n",
    "n2p2 = {k:v for k,v in model2.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = filter_params_to_merge([n for n in n2p1], ['.*classifier.*', '.*projection.bias', '.*layrnorm.*', '.*layernorm.*', '.*scale.*', '.*class_embedding', '.*position_embedding.*', '.*mask_token.*', '.*cls_token.*'])\n",
    "n2 = filter_params_to_merge([n for n in n2p2], ['.*classifier.*', '.*projection.bias', '.*layrnorm.*', '.*layernorm.*', '.*scale.*', '.*class_embedding', '.*position_embedding.*', '.*mask_token.*', '.*cls_token.*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = []\n",
    "for n in n2:\n",
    "    n_new.append(get_clip_name(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = sorted(n1)\n",
    "n_new = sorted(n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if n1 and n_new are the same\n",
    "for i in range(len(n1)):\n",
    "    if n1[i] != n_new[i]:\n",
    "        print(n1[i], n_new[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 == n_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision_model.encoder.layers.23.mlp.fc1.weight'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clip_name('encoder.layer.23.mlp.fc1.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_name(dinov2_name):\n",
    "    if 'embeddings.patch_embeddings.projection' in dinov2_name:\n",
    "        return dinov2_name.replace('embeddings.patch_embeddings.projection', 'vision_model.embeddings.patch_embedding')\n",
    "    if 'encoder.layer' in dinov2_name:\n",
    "        dinov2_name = dinov2_name.replace('encoder.layer', 'vision_model.encoder.layers')\n",
    "        if 'attention' in dinov2_name:\n",
    "            dinov2_name = dinov2_name.replace('attention', 'self_attn', 1)\n",
    "            if 'attention.query' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('attention.query', 'q_proj')\n",
    "            elif 'attention.key' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('attention.key', 'k_proj')\n",
    "            elif 'attention.value' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('attention.value', 'v_proj')\n",
    "            elif 'output.dense' in dinov2_name:\n",
    "                dinov2_name = dinov2_name.replace('output.dense', 'out_proj')\n",
    "            return dinov2_name\n",
    "        elif 'norm1' in dinov2_name:\n",
    "            return dinov2_name.replace('norm1', 'layer_norm1')\n",
    "        elif 'norm2' in dinov2_name:\n",
    "            return dinov2_name.replace('norm2', 'layer_norm2')\n",
    "        return dinov2_name\n",
    "    return dinov2_name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_merge(local_models, global_model, regmean_grams=None, **kwargs):\n",
    "    params = {}\n",
    "    for i, local_model in enumerate(local_models):\n",
    "        \n",
    "        n2p = {k: v for k,v in local_model.named_parameters()}\n",
    "        merge_param_names = filter_params_to_merge([n for n in n2p], ['.*classifier.*', '.*projection.bias', '.*layrnorm.*', '.*layernorm.*', '.*scale.*', '.*class_embedding', '.*position_embedding.*', '.*mask_token.*', '.*cls_token.*']) # for glue label spaces are different\n",
    "        for n in merge_param_names:\n",
    "            # if it is the second model, the second model should be dinov2\n",
    "            if i == 1:\n",
    "                n_clip = get_clip_name(n)\n",
    "                params[n_clip].append(n2p[n])\n",
    "            else:\n",
    "                if n not in params:\n",
    "                    params[n] = []\n",
    "                params[n].append(n2p[n])\n",
    "    \n",
    "    # we also need to modify the name of the parameters in the regmean grams\n",
    "    if regmean_grams:\n",
    "        for k in list(regmean_grams[1].keys()):\n",
    "            new_k = get_clip_name(k)\n",
    "            regmean_grams[1][new_k] = regmean_grams[1].pop(k)\n",
    "\n",
    "    if regmean_grams: # regmean average\n",
    "        avg_params = regmean_merge(params, regmean_grams)\n",
    "\n",
    "    else: # simple average\n",
    "        avg_params = {k: torch.stack(v,0).mean(0) for k, v in params.items()}\n",
    "\n",
    "    return avg_params\n",
    "\n",
    "def copy_params_to_model(avg_params, model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if n in avg_params:\n",
    "            p.data.copy_(avg_params[n])\n",
    "\n",
    "def reduce_non_diag(cov_mat, a):\n",
    "    diag_weight = torch.diag(torch.ones(cov_mat.size(0)) - a).to(cov_mat.device)\n",
    "    non_diag_weight = torch.zeros_like(diag_weight).fill_(a)\n",
    "    weight = diag_weight + non_diag_weight\n",
    "    ret = cov_mat * weight\n",
    "    return ret\n",
    "\n",
    "def regmean_merge(all_params, all_grams):\n",
    "    avg_params = {}\n",
    "    n_model = len(all_grams)\n",
    "    for name in all_params:\n",
    "        h_avged = False\n",
    "        if name.endswith('.weight'):\n",
    "            print(f'Regmean: {name}')\n",
    "            module_name = name[:-len('.weight')]\n",
    "            if module_name in all_grams[0]:\n",
    "                gram_m_ws, grams = [], []\n",
    "\n",
    "                for model_id, model_grams in enumerate(all_grams):\n",
    "                    param_grams = model_grams[module_name]\n",
    "\n",
    "                    # for roberta we dont need this; but it is important for deberta and t5\n",
    "                    #param_grams = reduce_non_diag(param_grams, a=0.9)\n",
    "\n",
    "                    param = all_params[name][model_id]\n",
    "                    gram_m_ws.append(torch.matmul(param_grams, param.transpose(0,1)))\n",
    "                    grams.append(param_grams)\n",
    "                sum_gram = sum(grams)\n",
    "                sum_gram_m_ws = sum(gram_m_ws)\n",
    "                sum_gram_inv = torch.inverse(sum_gram)\n",
    "                wt = torch.matmul(sum_gram_inv, sum_gram_m_ws)\n",
    "                w = wt.transpose(0,1)\n",
    "                avg_params[name] = w\n",
    "                h_avged = True\n",
    "        if not h_avged: # if not averaged with regmean, then do simple avg\n",
    "            avg_params[name] = torch.stack(all_params[name],0).mean(0)\n",
    "           \n",
    "    return avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regmean: vision_model.embeddings.patch_embedding.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.0.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.1.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.2.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.3.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.4.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.5.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.6.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.7.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.8.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.9.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.10.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.11.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.12.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.13.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.14.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.15.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.16.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.17.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.18.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.19.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.20.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.21.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.22.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.23.layer_norm2.weight\n"
     ]
    }
   ],
   "source": [
    "regmean_avg_params = avg_merge([model1, model2], merged_model, regmean_grams=[grams1, grams2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_params_to_model(regmean_avg_params, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13983.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/gavin/cambrian/cambrian/merge\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained('../../checkpoints/regmean_dinov2_clip_vit_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cambrian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
