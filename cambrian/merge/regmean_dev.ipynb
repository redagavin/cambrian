{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/gavin/miniconda3/envs/cambrian/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPVisionModel, Dinov2Model, TrainingArguments, Trainer\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_params_to_merge(param_names, exclude_param_regex):\n",
    "    params_to_merge = []\n",
    "    for name in param_names:\n",
    "        valid = not any([re.match(patt, name) for patt in exclude_param_regex])\n",
    "        if valid:\n",
    "            params_to_merge.append(name)\n",
    "    return params_to_merge\n",
    "\n",
    "\n",
    "def filter_modules_by_regex(base_module, include_patterns, include_type):\n",
    "    modules = {}\n",
    "    for name, module in base_module.named_modules():\n",
    "        valid_name = not include_patterns or any([re.match(patt, name) for patt in include_patterns])\n",
    "        valid_type = not include_type or any([isinstance(module, md_cls) for md_cls in include_type])\n",
    "        if valid_type and valid_name:\n",
    "            modules[name] = module\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ImageNet dataset\n",
    "# Note: You need to have your Hugging Face account set up and authenticated\n",
    "dataset = load_dataset(\"imagenet-1k\", split=\"train[:32000]\")  # Using validation set for quicker loading\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    if image.mode != 'RGB':\n",
    "        return image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "# Define image transformations\n",
    "transform_clip = transforms.Compose([\n",
    "    transforms.Lambda(convert_to_rgb),\n",
    "    transforms.Resize((336, 336)),  # Resize to match CLIP's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711])  # CLIP's normalization\n",
    "])\n",
    "\n",
    "# Define image transformations for DINOv2\n",
    "# DINOv2 uses a different input size and normalization compared to CLIP\n",
    "transform_dino = transforms.Compose([\n",
    "    transforms.Lambda(convert_to_rgb),\n",
    "    transforms.Resize((336, 336)),  # DINOv2 typically uses 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Create a custom dataset class\n",
    "class ImageNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {'pixel_values': image}\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "clip_dataset = ImageNetDataset(dataset, transform=transform_clip)\n",
    "clip_dataloader = DataLoader(clip_dataset, batch_size=512, shuffle=True, num_workers=32)\n",
    "\n",
    "dino_dataset = ImageNetDataset(dataset, transform=transform_dino)\n",
    "dino_dataloader = DataLoader(dino_dataset, batch_size=512, shuffle=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(model, dataloader):\n",
    "    grams = {} # gram matrices for each linear layer inputs\n",
    "    xn = {} # number of examples used for computing gram\n",
    "\n",
    "    def get_gram(name):\n",
    "        def hook(module, input, output):\n",
    "            x = input[0].detach() # $[b,t,h]\n",
    "            x = x.view(-1, x.size(-1))\n",
    "            xtx = torch.matmul(x.transpose(0,1), x) # [h,h]\n",
    "            if name not in grams:\n",
    "                grams[name] = xtx / x.size(0)\n",
    "                xn[name] = x.size(0)\n",
    "            else:\n",
    "                grams[name] = (grams[name] * xn[name] + xtx) / (x.size(0) + xn[name])\n",
    "                xn[name] += x.size(0)\n",
    "        return hook\n",
    "\n",
    "    linear_modules = filter_modules_by_regex(model, None, [nn.Linear])\n",
    "    handles = []\n",
    "    for name, module in linear_modules.items():\n",
    "        handle = module.register_forward_hook(get_gram(name))\n",
    "        handles.append(handle)\n",
    "\n",
    "    n_step = 30\n",
    "    total = n_step if n_step > 0 else len(dataloader)\n",
    "    for step, inputs in tqdm(enumerate(dataloader), total=total, desc='Computing gram matrix'):\n",
    "        if n_step > 0 and step == n_step:\n",
    "            break\n",
    "\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CLIPVisionModel.from_pretrained('../../checkpoints/dino2clip_vit').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gram matrix: 100%|██████████| 30/30 [14:25<00:00, 28.86s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grams1 = compute_gram(model1, clip_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing gram matrix: 100%|██████████| 30/30 [14:41<00:00, 29.38s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    grams2 = compute_gram(model2, dino_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_merge(local_models, global_model, regmean_grams=None, **kwargs):\n",
    "    params = {}\n",
    "    for local_model in local_models:\n",
    "        n2p = {k: v for k,v in local_model.named_parameters()}\n",
    "        merge_param_names = filter_params_to_merge([n for n in n2p], ['.*classifier.*']) # for glue label spaces are different\n",
    "        for n in merge_param_names:\n",
    "            if n not in params:\n",
    "                params[n] = []\n",
    "            params[n].append(n2p[n])\n",
    "\n",
    "    if regmean_grams: # regmean average\n",
    "        avg_params = regmean_merge(params, regmean_grams)\n",
    "\n",
    "    else: # simple average\n",
    "        avg_params = {k: torch.stack(v,0).mean(0) for k, v in params.items()}\n",
    "\n",
    "    return avg_params\n",
    "\n",
    "def copy_params_to_model(avg_params, model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if n in avg_params:\n",
    "            p.data.copy_(avg_params[n])\n",
    "\n",
    "def reduce_non_diag(cov_mat, a):\n",
    "    diag_weight = torch.diag(torch.ones(cov_mat.size(0)) - a).to(cov_mat.device)\n",
    "    non_diag_weight = torch.zeros_like(diag_weight).fill_(a)\n",
    "    weight = diag_weight + non_diag_weight\n",
    "    ret = cov_mat * weight\n",
    "    return ret\n",
    "\n",
    "def regmean_merge(all_params, all_grams):\n",
    "    avg_params = {}\n",
    "    n_model = len(all_grams)\n",
    "    for name in all_params:\n",
    "        h_avged = False\n",
    "        if name.endswith('.weight'):\n",
    "            print(f'Regmean: {name}')\n",
    "            module_name = name[:-len('.weight')]\n",
    "            if module_name in all_grams[0]:\n",
    "                gram_m_ws, grams = [], []\n",
    "\n",
    "                for model_id, model_grams in enumerate(all_grams):\n",
    "                    param_grams = model_grams[module_name]\n",
    "\n",
    "                    # for roberta we dont need this; but it is important for deberta and t5\n",
    "                    #param_grams = reduce_non_diag(param_grams, a=0.9)\n",
    "\n",
    "                    param = all_params[name][model_id]\n",
    "                    gram_m_ws.append(torch.matmul(param_grams, param.transpose(0,1)))\n",
    "                    grams.append(param_grams)\n",
    "                sum_gram = sum(grams)\n",
    "                sum_gram_m_ws = sum(gram_m_ws)\n",
    "                sum_gram_inv = torch.inverse(sum_gram)\n",
    "                wt = torch.matmul(sum_gram_inv, sum_gram_m_ws)\n",
    "                w = wt.transpose(0,1)\n",
    "                avg_params[name] = w\n",
    "                h_avged = True\n",
    "        if not h_avged: # if not averaged with regmean, then do simple avg\n",
    "            avg_params[name] = torch.stack(all_params[name],0).mean(0)\n",
    "           \n",
    "    return avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regmean: vision_model.embeddings.patch_embedding.weight\n",
      "Regmean: vision_model.embeddings.position_embedding.weight\n",
      "Regmean: vision_model.pre_layrnorm.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.0.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.0.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.1.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.1.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.2.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.2.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.3.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.3.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.4.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.4.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.5.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.5.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.6.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.6.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.7.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.7.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.8.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.8.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.9.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.9.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.10.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.10.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.11.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.11.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.12.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.12.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.13.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.13.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.14.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.14.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.15.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.15.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.16.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.16.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.17.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.17.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.18.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.18.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.19.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.19.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.20.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.20.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.21.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.21.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.22.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.22.layer_norm2.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "Regmean: vision_model.encoder.layers.23.layer_norm1.weight\n",
      "Regmean: vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "Regmean: vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "Regmean: vision_model.encoder.layers.23.layer_norm2.weight\n",
      "Regmean: vision_model.post_layernorm.weight\n"
     ]
    }
   ],
   "source": [
    "regmean_avg_params = avg_merge([model1, model2], merged_model, regmean_grams=[grams1, grams2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_params_to_model(regmean_avg_params, merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "merged_model.save_pretrained('../../checkpoints/regmean_dinov2_clip_vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cambrian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
